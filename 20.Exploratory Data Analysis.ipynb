{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e040225",
   "metadata": {},
   "source": [
    "The merged database is made of fairly lot of features, I will explore them and see if some could be optimized, using two technics: \n",
    "\n",
    "- Graphical [Scatter Plots](21.Scatter%20plots%20for%20EDA.ipynb) between numerical features and the dependent variable.\n",
    "\n",
    "- Using [PCA and Scree plot](22.PCA%20and%20Screeplot.ipynb).\n",
    "\n",
    "Before going further, I've taken time to write a new function into [my_utils](my_utils.py) library to facilitate the loading of datasets from the NPZ backup file created in the previous [notebook](17.The%20global%20Dataset%20-%20Merging%20all%20the%20datasets%20into%20a%20big%20one.ipynb)\n",
    "\n",
    "- load_npz_as_dict()\n",
    "- load_dataset()\n",
    "- load_Xy()\n",
    "\n",
    "Code of those functions is inspired by the code written in my [my_lib.py library](https://github.com/epfl-extension-school/project-adsml19-c4-s11-3871-2111/blob/master/mylib.py) while completing my [course #4 project](https://github.com/xnicolovici/machine_learning/tree/master/notebooks/Course%20No%204/11.%20Course%20project), and the function *duildDataMatrix()* written in [House Price model training chapter](https://github.com/epfl-extension-school/project-adsml19-c3-s9-3871-2111/blob/master/house-prices/house-prices-solution-2-of-2.ipynb) while completing [course #3 project](https://github.com/xnicolovici/machine_learning/tree/master/notebooks/Course%20No%203/09.%20Course%20project).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e50b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening connection to database\n",
      "Add pythagore() function to SQLite engine\n",
      "Fraction of the dataset used to train models: 10.00%\n",
      "my_utils library loaded :-)\n"
     ]
    }
   ],
   "source": [
    "# Load my_utils.ipynb in Notebook\n",
    "from ipynb.fs.full.my_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78912037",
   "metadata": {},
   "source": [
    "# load_npz_as_dict()\n",
    "\n",
    "Remember, in the previous [notebook](17.The%20global%20Dataset%20-%20Merging%20all%20the%20datasets%20into%20a%20big%20one.ipynb), I've stored all the datasets into an NPZ file, to easyly reload them from disk.\n",
    "\n",
    "The *load_npz_as_dict()* function aims to return a *dict* Python object filled in with the datasets loaded from an NPZ file. The function expects two main parameters:\n",
    "- filename, the path to the NPZ file on disk, default set to *NPZ_NORMALIZED_DATAFILE* constant.\n",
    "- dataset, the name of the dataset I'd like to load, default being hte 'full' one. Others are *stations*, *travel*, *weather_num* and *weather_cat*\n",
    "\n",
    "Two other optional parameters are available:\n",
    "- frac, a value between 0 and 1 used to get a sample of the dataset requested, 1 being 100% and 0 none. This parameter will be usefull when coding and training model to work on small subset of the data, 1.5 millions of line and 48 features might be too heavy to be processed on my desktop computer, even if it runs Apple M1 Silicon ;-)\n",
    "- verbose, a boolean parameter, simply ask the function to display some informations when its value equal *True*\n",
    "\n",
    "When used, this function opens the NPZ file, loads the requested dataset, convert them to a *pandas.DataFrame* object using column names loaded from the NPZ file, add it to a *dict()* Python object along with the *frac* parameter value and, if the requested dataset is the *full* one, add to the *dict()* object the feature name details of this *full* dataset (numerical, categorical, y and all).\n",
    "\n",
    "This utility function will be used each time I need to get one of the *engineered* dataset, using the *frac* parameter to work on subset of it.\n",
    "\n",
    "> Note: When verbose=True, this function display the *shape* of the dataset returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59293759",
   "metadata": {},
   "source": [
    "## Header of *load_npz_as_dict()*\n",
    "implementation can be read in [my_utils](my_utils.ipynb) libary.\n",
    "\n",
    "    def load_npz_to_dict(filename=NPZ_DATAFILE, dataset='full', frac=1, verbose=True) -> dict:\n",
    "        \"\"\"\n",
    "        This function returns one of the dataset stored in the NPZ file passed as parameter,\n",
    "        and if the dataset claimed is the full one, then its feature names are added to the\n",
    "        dict returned by the function.\n",
    "\n",
    "        The dict structure returned looks like this:\n",
    "            - feature_names: (if requested dataset is the full one)\n",
    "                - numerical\n",
    "                - categorrical\n",
    "                - all\n",
    "                - y\n",
    "            - dataset\n",
    "            - frac  \n",
    "\n",
    "        The NPZ file passed should contain a Python dict built in Notebook No 17\n",
    "\n",
    "        The dataset parameter is used to determine which dataset the function should return.\n",
    "        Dafault value is 'full'\n",
    "\n",
    "        The frac parameter\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "\n",
    "        \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8df2c",
   "metadata": {},
   "source": [
    "## Demonstration of the *load_npz_to_dict()* function\n",
    "\n",
    "I'd like to load 10% of the *stations* dataset and display the first three lines.\n",
    "\n",
    "As the *stations* dataset contains 83 lines and 5 columns, I should obtain a dataset with shape=(8, 5), 10% of 83 lines with 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2c2052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'stations' from NPZ file ./data/capstone-data-normalized.npz\n",
      "Building sample from dataset (frac=0.1)\n",
      " Dataset shape: (8, 5)\n",
      "\n",
      "\n",
      "Dataset loaded, returning dict\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>USC00282023</td>\n",
       "      <td>CRANFORD, NJ US</td>\n",
       "      <td>40.6666</td>\n",
       "      <td>-74.3235</td>\n",
       "      <td>24.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>US1NJHD0002</td>\n",
       "      <td>KEARNY 1.7 NW, NJ US</td>\n",
       "      <td>40.7729</td>\n",
       "      <td>-74.1409</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>US1NJUN0014</td>\n",
       "      <td>WESTFIELD 0.6 NE, NJ US</td>\n",
       "      <td>40.6588</td>\n",
       "      <td>-74.3358</td>\n",
       "      <td>36.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        STATION                     NAME LATITUDE LONGITUDE ELEVATION\n",
       "58  USC00282023          CRANFORD, NJ US  40.6666  -74.3235      24.4\n",
       "25  US1NJHD0002     KEARNY 1.7 NW, NJ US  40.7729  -74.1409        29\n",
       "14  US1NJUN0014  WESTFIELD 0.6 NE, NJ US  40.6588  -74.3358      36.3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz_dict=load_npz_as_dict(dataset='stations', frac=0.1)\n",
    "df=npz_dict['dataset']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe73c99",
   "metadata": {},
   "source": [
    "# load_dataset()\n",
    "\n",
    "Along with the *load_npz_to_dict()* function, I've written a wrapper function around it: *load_dataset()*\n",
    "\n",
    "This wrapper function returns a tuple instead of a dict for code simplification. For example, the following instruction loads the full dataset, store it in *df* variable, and initialize a *feature* variable that contains a list of the rest of the values of the returned tuple.\n",
    "    \n",
    "        df,*features=load_dataset()\n",
    "\n",
    "Be aware that this function removed from the retunred features column name list the name of the result vector column. This is a big difference with *load_npz_as_dict()* function which returns **all** dataframe column names in a single list (*load_npz_as_dict()['features']['all']*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79eeac",
   "metadata": {},
   "source": [
    "## Header of *load_dataset()*\n",
    "implementation can be read in [my_utils](my_utils.ipynb) libary.\n",
    "\n",
    "    def load_dataset(frac=1, random_state=5, verbose=True, y_dtype='float', npz_filename=NPZ_DATAFILE) -> tuple:\n",
    "        \"\"\"\n",
    "        Convenient wrapper around the load_npz_as_dict() function that returns the full dataset, its feature and result vector name\n",
    "        as a tuple.\n",
    "\n",
    "        This function exists to simplify the code when loading full dataset. For example, the follwing instruction\n",
    "        loads the full dataset store it in df variable, feature variable will contain a list of the rest of the tuple\n",
    "\n",
    "            df,*features=load_dataset()\n",
    "\n",
    "        Parameters are passed as is to the load_npz_as_dict() function.\n",
    "\n",
    "        Returned tuple is:\n",
    "            - dataset\n",
    "            - all feature names\n",
    "            - y result vector name\n",
    "            - numerical feature names\n",
    "            - categorical feature names\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "\n",
    "        \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ad06d",
   "metadata": {},
   "source": [
    "## Demonstration of the *load_dataset()* function\n",
    "\n",
    "I'd like to load 10% of the *full* dataset and display the feature names and the first three lines of the dataset returned:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e240f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column names:\n",
      "pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,store_and_fwd_flag,weekend,day_period_afternoon,day_period_evening,day_period_morning,passenger_alone,diff_ELEVATION,diff_ASCENDING,diff_DESCENDING,WC_WT01,WC_WT02,WC_WT03,WC_WT04,WC_WT06,WC_WT08,WC_WT09,WC_WT11,WC_WDIR_E,WC_WDIR_N,WC_WDIR_NE,WC_WDIR_NW,WC_WDIR_S,WC_WDIR_SE,WC_WDIR_SW,WC_WDIR_W,WC_PEAK_Y,WC_SNOW_FALL,WC_SNOW_ROAD,distance_in_km_square_log10,dropoff_distance_to_STATION_log1p,WNP_AWND_log1p,WNP_SNWD_log1p,WND_AWND_log1p,WND_SNWD_log1p\n",
      "\n",
      "First three line of dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>weekend</th>\n",
       "      <th>day_period_afternoon</th>\n",
       "      <th>day_period_evening</th>\n",
       "      <th>day_period_morning</th>\n",
       "      <th>km_per_hour</th>\n",
       "      <th>passenger_alone</th>\n",
       "      <th>diff_ELEVATION</th>\n",
       "      <th>diff_ASCENDING</th>\n",
       "      <th>diff_DESCENDING</th>\n",
       "      <th>WC_WT01</th>\n",
       "      <th>WC_WT02</th>\n",
       "      <th>WC_WT03</th>\n",
       "      <th>WC_WT04</th>\n",
       "      <th>WC_WT06</th>\n",
       "      <th>WC_WT08</th>\n",
       "      <th>WC_WT09</th>\n",
       "      <th>WC_WT11</th>\n",
       "      <th>WC_WDIR_E</th>\n",
       "      <th>WC_WDIR_N</th>\n",
       "      <th>WC_WDIR_NE</th>\n",
       "      <th>WC_WDIR_NW</th>\n",
       "      <th>WC_WDIR_S</th>\n",
       "      <th>WC_WDIR_SE</th>\n",
       "      <th>WC_WDIR_SW</th>\n",
       "      <th>WC_WDIR_W</th>\n",
       "      <th>WC_PEAK_Y</th>\n",
       "      <th>WC_SNOW_FALL</th>\n",
       "      <th>WC_SNOW_ROAD</th>\n",
       "      <th>distance_in_km_square_log10</th>\n",
       "      <th>dropoff_distance_to_STATION_log1p</th>\n",
       "      <th>WNP_AWND_log1p</th>\n",
       "      <th>WNP_SNWD_log1p</th>\n",
       "      <th>WND_AWND_log1p</th>\n",
       "      <th>WND_SNWD_log1p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.073954</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.351326</td>\n",
       "      <td>0.931212</td>\n",
       "      <td>1.987874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.987874</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991388</td>\n",
       "      <td>1</td>\n",
       "      <td>37.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.513198</td>\n",
       "      <td>1.838520</td>\n",
       "      <td>1.526056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.173514</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.034316</td>\n",
       "      <td>1</td>\n",
       "      <td>37.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.610335</td>\n",
       "      <td>1.444618</td>\n",
       "      <td>1.774952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.87168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  store_and_fwd_flag  \\\n",
       "0        -73.982155        40.767937         -73.964630         40.765602                   0   \n",
       "1        -73.980415        40.738564         -73.999481         40.731152                   0   \n",
       "2        -73.979027        40.763939         -74.005333         40.710087                   0   \n",
       "\n",
       "   weekend  day_period_afternoon  day_period_evening  day_period_morning  km_per_hour  \\\n",
       "0        0                     1                   0                   0     1.073954   \n",
       "1        1                     0                   0                   0     0.991388   \n",
       "2        0                     0                   0                   1     1.034316   \n",
       "\n",
       "   passenger_alone  diff_ELEVATION  diff_ASCENDING  diff_DESCENDING  WC_WT01  WC_WT02  WC_WT03  \\\n",
       "0                1             0.0               0                0        1        0        0   \n",
       "1                1            37.2               0                1        0        0        0   \n",
       "2                1            37.2               0                1        0        0        0   \n",
       "\n",
       "   WC_WT04  WC_WT06  WC_WT08  WC_WT09  WC_WT11  WC_WDIR_E  WC_WDIR_N  WC_WDIR_NE  WC_WDIR_NW  \\\n",
       "0        0        0        1        0        0          1          0           1           0   \n",
       "1        0        0        1        0        0          0          0           0           1   \n",
       "2        0        0        0        0        0          0          0           0           1   \n",
       "\n",
       "   WC_WDIR_S  WC_WDIR_SE  WC_WDIR_SW  WC_WDIR_W  WC_PEAK_Y  WC_SNOW_FALL  WC_SNOW_ROAD  \\\n",
       "0          0           0           0          0          1             0             0   \n",
       "1          0           0           0          1          1             0             0   \n",
       "2          0           0           0          1          1             1             1   \n",
       "\n",
       "   distance_in_km_square_log10  dropoff_distance_to_STATION_log1p  WNP_AWND_log1p  WNP_SNWD_log1p  \\\n",
       "0                     0.351326                           0.931212        1.987874             0.0   \n",
       "1                     0.513198                           1.838520        1.526056             0.0   \n",
       "2                     1.610335                           1.444618        1.774952             0.0   \n",
       "\n",
       "   WND_AWND_log1p  WND_SNWD_log1p  \n",
       "0        1.987874         0.00000  \n",
       "1        1.173514         0.00000  \n",
       "2        0.000000         2.87168  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df,x_column,*_=load_dataset(verbose=False)\n",
    "\n",
    "print(\"Feature column names:\")\n",
    "print(','.join(x_column))\n",
    "\n",
    "print(\"\\nFirst three line of dataset:\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1eb185",
   "metadata": {},
   "source": [
    "Cool, isn't it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897111d7",
   "metadata": {},
   "source": [
    "# load_Xy_as_dict()\n",
    "\n",
    "Training model is based on *np.array* of features (X) and vector result (y) build from datasets, splitted in train and validation subsets.\n",
    "\n",
    "I've coded the *load_Xy_as_dict()* function to do the job in one call:\n",
    "\n",
    "- Load the full dataset using *load_dataset()*\n",
    "- Split dataframe in two subset, *train* and *valid*, using *sklearn.model_selection.train_test_split()* method\n",
    "- Print some information on the shape of the subset created (if verbose=True)\n",
    "- Return a dict of the different *np.arrays* created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6422f2a",
   "metadata": {},
   "source": [
    "## Header of *load_Xy_as_dict()*\n",
    "implementation can be read in [my_utils](my_utils.ipynb) libary.\n",
    "\n",
    "    def load_Xy(train_size=TRAIN_SIZE_DEFAULT, frac=1, random_state=5, verbose=True, npz_filename=NPZ_DATAFILE) -> dict:\n",
    "        \"\"\"\n",
    "        Used to get features and vector result of the 'full' dataset as X and y np.array, splitted into two daatset: A train\n",
    "        and valid one.\n",
    "\n",
    "        The 'train_size' parameter may be used to fix the train size (defaults 0.8). This parameter is passed as is to the\n",
    "        'sklearn.model_selection.train_test_split()' method.\n",
    "\n",
    "        The value returned is a dict object:\n",
    "\n",
    "            - train:\n",
    "                - X:    Train set of X features\n",
    "                - y:    Train set of y vector result\n",
    "\n",
    "            - valid:\n",
    "                - X:    Validation set of X features\n",
    "                - y:    Validation set of y vector result\n",
    "\n",
    "            - all:\n",
    "                - X:    Complete set of X features\n",
    "                - y:    Complete set of y vector result\n",
    "\n",
    "            - features: List of feature names\n",
    "            - result:   Name of the y result vector\n",
    "\n",
    "        The 'full' dataset is retrived using the 'load_dataset()' function.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "\n",
    "        \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b80cf",
   "metadata": {},
   "source": [
    "## Demonstration of the *load_Xy_as_dict()* function\n",
    "\n",
    "I'd like to load 1% of the X train feature values from the *full* dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e915e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the X train dataset using 1% of the full dataset: (11411, 38)\n"
     ]
    }
   ],
   "source": [
    "X_tr=load_Xy_as_dict(frac=0.01, verbose=False)['train']['X']\n",
    "\n",
    "print(\"Shape of the X train dataset using 1% of the full dataset:\", X_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44826b",
   "metadata": {},
   "source": [
    "# load_Xy()\n",
    "\n",
    "A wrapper function around *load_Xy_as_dict()* that returns train and valid X/y values as a tuple, to simplify code in next Notebooks.\n",
    "\n",
    "More informations below in the header of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1aef7",
   "metadata": {},
   "source": [
    "## Header of *load_Xy()*\n",
    "implementation can be read in [my_utils](my_utils.ipynb) libary.\n",
    "\n",
    "    def load_Xy(train_size=TRAIN_SIZE_DEFAULT, frac=1, random_state=5, verbose=True, npz_filename=NPZ_DATAFILE) -> tuple:\n",
    "        \"\"\"\n",
    "        A wrapper function around load_Xy_as_dict() that returns X_tr, y_tr, X_va and y_va as a tuple.\n",
    "\n",
    "        This function aims to simplify the code in Notebooks\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        (X_tr, y_tr, X_va, y_va)\n",
    "\n",
    "        \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee6ce04",
   "metadata": {},
   "source": [
    "## Demonstration of the *load_Xy()* function\n",
    "\n",
    "I'd like to load 1% of the train and validation feature values of the *full* dataset in one instruction, with train/valid split = 60%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d620129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr shape (8558, 38)\n",
      "y_tr shape (8558,)\n",
      "X_va shape (5706, 38)\n",
      "y_va shape (5706,)\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr, X_va, y_va = load_Xy(frac=0.01, train_size=0.6)\n",
    "print(\"X_tr shape\", X_tr.shape)\n",
    "print(\"y_tr shape\", y_tr.shape)\n",
    "print(\"X_va shape\", X_va.shape)\n",
    "print(\"y_va shape\", y_va.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b812d0f",
   "metadata": {},
   "source": [
    "# Let's continue\n",
    "to the next notebook, [Scatter plots for EDA](21.Scatter%20plots%20for%20EDA.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
