{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1dc911",
   "metadata": {},
   "source": [
    "# Time to train models :-)\n",
    "\n",
    "I've made the choice of training four different types of Machine Learning models and compare results between them:\n",
    "\n",
    "- Gradient Descent based: [Ridge Regressor](31.Gradient%20Descent%20Based%20-%20Ridge%20Regressor.ipynb)\n",
    "\n",
    "- Distance based: [KNeighborsRegressor](32.Distance%20Based%20-%20KNeighborsRegressor.ipynb)\n",
    "\n",
    "- Category based: [RandomForestRegressor](33.Category%20Based%20-%20RandomForestRegressor.ipynb)\n",
    "\n",
    "- Neural network: [MLPRegressor](34.Neural%20Network%20-%20MLPRegressor.ipynb)\n",
    "\n",
    "\n",
    "Due to the quite big dataset I have, I will use a [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) step with large parameter scopes to identify the best ML model parameter intervals, and refine the search using reduced scopes with a [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). To do preprocessing using scaler like [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) and/or component reduction with [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), I will combine grid search with [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "The combination of [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) is an approach that I've already used in my [course #3 project](https://github.com/epfl-extension-school/project-adsml19-c3-s9-3871-2111/blob/master/house-prices/house-prices-solution-2-of-2.ipynb), inspired by this article: [SKlearn: Pipeline & GridSearchCV](https://medium.com/@cmukesh8688/sklearn-pipeline-gridsearchcv-54f5552bbf4e).\n",
    "\n",
    "Before going to train. models, let me expose some tricks and functions that will be used when training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8479198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening connection to database\n",
      "Add pythagore() function to SQLite engine\n",
      "Fraction of the dataset used to train models: 10.00%\n",
      "my_utils library loaded :-)\n"
     ]
    }
   ],
   "source": [
    "# Load my_utils.ipynb in Notebook\n",
    "from ipynb.fs.full.my_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce274ee",
   "metadata": {},
   "source": [
    "# Work on a fraction of the datasets\n",
    "\n",
    "The *full* dataset I've built to train models is made of about 1.5 millions of lines and 50 features.\n",
    "\n",
    "In order to speed-up model training, I will work on a fraction of this dataset. To have the same value accross all the Notebooks of this fraction parameter, I've coded it as a *CONSTANT* in [my_utils](my_utils.ipynb) library.\n",
    "\n",
    "The current value is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ad818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of the dataset used to train models: 10.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of the dataset used to train models: {:.2f}%\".format(FRAC_VALUE_FOR_ML*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3bdce1",
   "metadata": {},
   "source": [
    "# *Mean Absolute Error* and *Mean Absolute Percent Error* to evaluate model performance.\n",
    "\n",
    "I've choosen the [mean_absolute_error()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) approach from *sklearn.metrics* to evaluate my models performance. It helps to determine the *average* error made by models on prediction.\n",
    "\n",
    "In order to simplify the use of this [mean_absolute_error()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) in the next Notebooks, I've coded two functions: *mae()* and *mape()*\n",
    "\n",
    "## mae(): Mean Absolute Error\n",
    "\n",
    "This function returns the *Mean Absolute Error* of a prediction in km/h. It takes care of the fact that the result vector of the *Full* dataset, *km_per_hour*, has been log transformed.\n",
    "\n",
    "    def mae(y_pred, y) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns 10^mean_absolute_error() between the two result\n",
    "        vector passed as parameter.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        10^(mean absolute error)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return 10**mean_absolute_error(y_pred, y)\n",
    "\n",
    "## mape(): Mean Absolute Percent Error\n",
    "\n",
    "This function returns the *Mean Absolute Error* expressed in percentage, 100% representing a perfect prediction (without any errors).\n",
    "\n",
    "Expressing the score of prediction in percentage is an advantage as it takes care of the context. Saying that the model has a MAPE of 88% gives more representative information on the model performance than saying that MAE is 1.5 km/h.\n",
    "\n",
    "    def mape(y_pred, y) -> float:\n",
    "        \"\"\"\n",
    "        Define a performance metric in percentage\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        mean absolute percentage score\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Return percentage value\n",
    "        return 100 - np.mean(100 * (mean_absolute_error(y_pred, y) / y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4ec5e",
   "metadata": {},
   "source": [
    "# Define a baseline\n",
    "\n",
    "In order to evaluate our models, one quite easy and direct method would be to compare them to a simple baseline built with *sklearn.dummy.DummyRegressor*\n",
    "\n",
    "As I've decided to evaluate my models against the *mean absolute error*, let's use this *DummyRegressor* with parameter *strategy='mean'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0237678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier accuracy in km/h       : 1.48 km/h\n",
      "Dummy classifier accuracy in percentage : 84.02 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Load X and y dataset\n",
    "X_tr, y_tr, X_va, y_va=load_Xy(frac=FRAC_VALUE_FOR_ML)\n",
    "\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "dummy.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred=dummy.predict(X_va)\n",
    "\n",
    "print(\"Dummy classifier accuracy in km/h       : {:.2f} km/h\".format(mae(y_pred, y_va)))\n",
    "print(\"Dummy classifier accuracy in percentage : {:.2f} %\".format(mape(y_pred, y_va)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5871383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model dummy to ./data/model-dummy.sav using 'pickle' library\n"
     ]
    }
   ],
   "source": [
    "# Save model for later use\n",
    "save_model(model=dummy, name='dummy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997b832",
   "metadata": {},
   "source": [
    "# GridSearchCV scoring function\n",
    "\n",
    "GridSearchCV use internal scoring function to determine prediction performance for each parameters tested, and performs a classification of the parameters combination to determine the best one.\n",
    "\n",
    "I've decided to not use this internal scoring function (which by defaut is an *R2* function for regression) and define my own scoring function using [sklearn.metrics.make_scorer](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) and the *mape()* function defined above.\n",
    "\n",
    "The two following lines will be added to [my_utils](my_utils.ipynb) library to make it available in ML training Notebooks.\n",
    "\n",
    "Using this scoring function will let me draw more understable train graphs as the results on the Y-axis will be expressed in *percentage of performance*.\n",
    "\n",
    "    from sklearn.metrics import make_scorer\n",
    "    custom_scorer = make_scorer(mape, greater_is_better=True)\n",
    "\n",
    "> Note: The *greater_is_best* parameter to *True* will instruct the GridSearchCV objects that best result is the one with the higher *MAPE* value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70220a3",
   "metadata": {},
   "source": [
    "# GridSearchCV results plotting function\n",
    "\n",
    "As I've decided to use *RandomizedSearchCV* and *GridSearchCV* to tune some hyperparameters, I've reimplemented a function initially coded during my [cource #4 project](https://github.com/epfl-extension-school/project-adsml19-c4-s11-3871-2111/blob/master/mylib.py) used to draw the results from the search (using *xSearchCV()cv_results_* property).\n",
    "\n",
    "This will help to visualy seek for the best hyperparameters.\n",
    "\n",
    "Header of the function is copied below, implementation is available in [my_utils](my_utils.ipynb) library.\n",
    "\n",
    "    def plot_grid_search_results(results_df,\n",
    "                                 x_param,\n",
    "                                 y_param=['mean_test_score', 'mean_train_score'],\n",
    "                                 semilogx=True,\n",
    "                                 xlabel='',\n",
    "                                 ylabel='Score (%)',\n",
    "                                 title='GridSearch results',\n",
    "                                 figsize=(15,10),\n",
    "                                 std_params={'mean_test_score': 'std_test_score'},\n",
    "                                 std_factor=1,\n",
    "                                 show_best_result=['mean_test_score'],\n",
    "                                 greater_is_best=False\n",
    "                                ) -> None:\n",
    "        \"\"\"\n",
    "        Function to graph data points from GridSearchCV results., used to graph\n",
    "        the mean test score of a GridSearchCV fitted object.\n",
    "\n",
    "        Mandatory parameters are:\n",
    "            results_df: A dataframe built from GridSearchCV.cv_results_ property\n",
    "            x_param: The column name of the results_df dataframe to be used as X axis\n",
    "            y_param: An array of column to be plotted on the Y axis.\n",
    "\n",
    "        Optionnal parameters:\n",
    "            semilogx: If True, the X data points are plotted using a log10 scale\n",
    "            xlabel: Label of the X axis\n",
    "            ylabel: Label of the Y axis\n",
    "            title: Title of the graph\n",
    "            figsize: Size ot the graph\n",
    "            std_param: A dict with key=y_param element and value the corresponding\n",
    "                       standard deviation column name.\n",
    "                       This parameters is used to draw the std deviation of the\n",
    "                       y_params as a filled area around the data plot\n",
    "            std_factor: This parameter is used to amplify the standard deviation\n",
    "                        when building the std dev filled area. Default value is 1 and\n",
    "                        changing increasing it allows displaying standard 'small'\n",
    "                        deviation behaviours.\n",
    "                        Be warn that when changing this parameter to a value other\n",
    "                        that 1, the filled area does not represent absolute values\n",
    "                        but a trend of it.\n",
    "\n",
    "        The function will also determine, for each of the y_param to be plotted,\n",
    "        which is the plot with the highest y_param value, and use the coordinates\n",
    "        to draw a red cross on the plotted line, along with horizontal and vertical\n",
    "        lines to the X and Y axis.\n",
    "\n",
    "        For that purpose, the function first sort the results_df dataframe using\n",
    "        the x_param column in ascending order.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86883771",
   "metadata": {},
   "source": [
    "# Functions to save and load fitted models\n",
    "\n",
    "Same as I've done in my [course #4 project](https://github.com/epfl-extension-school/project-adsml19-c4-s11-3871-2111), I will save the train models on disk using *pickle.dump()* method.\n",
    "\n",
    "To do so, I've coded into [my_utils](my_utils.ipynb) two functions, *get_model_filename()*, *save_model()* and *load_model()*\n",
    "\n",
    "## Function headers\n",
    "\n",
    "### get_model_filename()\n",
    "\n",
    "    def get_model_filename(model_name) -> str:\n",
    "        \"\"\"\n",
    "        Basic function that will return the filename used to store on disk\n",
    "        the model passed as parameter\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "### save_model()\n",
    "\n",
    "    def save_model(model, name) -> None:\n",
    "        \"\"\"\n",
    "        Function that saves on disk the fitted model passed as first\n",
    "        parameter using pickle or keras library, depending on model type\n",
    "        It uses the function getModelFilename() with the 'name'\n",
    "        parameter to get the filename where to save the model.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "### load_model()\n",
    "\n",
    "    def load_model(name):\n",
    "        \"\"\"\n",
    "        Function that loads from disk the model of which name is passed\n",
    "        as first parameter. It uses the function getModelFilename() with\n",
    "        the 'name' parameter to get the filename from where to load the model.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Fitted model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "## How to use those functions ?\n",
    "\n",
    "Saving a trained model would ba as easy as:\n",
    "\n",
    "    save_model('model_name', <fitted_model>)\n",
    "\n",
    "Loading it will be:\n",
    "\n",
    "    model=load_model('model_name')\n",
    "    \n",
    "Models are save on disk, in the *data* directory with the following pattern:\n",
    "\n",
    "    model-<model_name>.sav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555467c",
   "metadata": {},
   "source": [
    "# Time to go...\n",
    "\n",
    "To the first model: [Gradient Descent Based - Ridge Regressor](31.Gradient%20Descent%20Based%20-%20Ridge%20Regressor.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
