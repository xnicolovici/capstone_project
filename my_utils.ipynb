{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some globally used libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3 as sql\n",
    "import seaborn as sns\n",
    "import geopandas\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import zscore\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder\n",
    "DATA_PATH=os.path.join('.','data')\n",
    "\n",
    "# Path to the image folder\n",
    "IMG_PATH=os.path.join('.','img')\n",
    "\n",
    "# SQLLite database filename\n",
    "SQLITE_DATAFILE=os.path.join(DATA_PATH,'capstone-db.sqlite')\n",
    "\n",
    "# NPZ data filename\n",
    "NPZ_DATAFILE=os.path.join(DATA_PATH, 'capstone-data.npz')\n",
    "\n",
    "# NPZ normalized data filename\n",
    "NPZ_NORMALIZED_DATAFILE=os.path.join(DATA_PATH, 'capstone-data-normalized.npz')\n",
    "\n",
    "# Tablename of the final dataset\n",
    "STATION_TABLENAME='stations'\n",
    "TRAVEL_TABLENAME='travel_improved'\n",
    "WEATHER_CAT_TABLENAME='weather_cat_improved'\n",
    "WEATHER_NUM_TABLENAME='weather_num_improved'\n",
    "\n",
    "# Pandas library default options\n",
    "PANDAS_DISPLAY_WIDTH=100\n",
    "PANDAS_DISPLAY_MAX_COLUMN=50\n",
    "PANDAS_DISPLAY_MAX_ROWS=15\n",
    "\n",
    "# Set default value for train dataset size\n",
    "TRAIN_SIZE_DEFAULT=0.8\n",
    "\n",
    "# Frac parameter used when loading data for ML training\n",
    "FRAC_VALUE_FOR_ML=0.1\n",
    "\n",
    "# Random state value passed to fix rendom results\n",
    "RANDOM_STATE=47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some default display for the pandas lib\n",
    "pd.set_option(\"display.width\", PANDAS_DISPLAY_WIDTH)\n",
    "pd.set_option(\"display.max_columns\", PANDAS_DISPLAY_MAX_COLUMN)\n",
    "pd.set_option(\"display.max_rows\", PANDAS_DISPLAY_MAX_ROWS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(dataset = 'travel', low_memory=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Open csv file passed as parameter from {DATA_PATH} folder.\n",
    "    This function will take care of splitted dataset like nyc_travel_X.csv\n",
    "    and will rebuild a complete dataset by merging multiple files.\n",
    "    \n",
    "    This function will seek for files on disk using the following algorythm:\n",
    "      - \"nyc_{}_{}.csv\".format(dataset,<part_number>) with part number varying depending on\n",
    "        the number of dataset split.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "    \"\"\"    \n",
    "    \n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.read_csv(f'{DATA_PATH}/{filename}', low_memory=low_memory) \n",
    "            for filename in os.listdir(DATA_PATH) \n",
    "            if (\n",
    "                filename.endswith('.csv') and\n",
    "                filename.startswith(f'nyc_{dataset}')\n",
    "            )\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening connection to database\n"
     ]
    }
   ],
   "source": [
    "print(\"Opening connection to database\")\n",
    "sql_database_connection=sql.connect(SQLITE_DATAFILE)\n",
    "\n",
    "\n",
    "def get_sql_connection() -> sql.Connection:\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns SQLite connection object to the SQL database file of the project\n",
    "    \n",
    "    This function shouldn't be used directly, it's a convenient function to be used by\n",
    "    load_sql() and save_sql() functions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sqlite3.Connection\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    # Return SQLite connection object\n",
    "    return sql_database_connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add pythagore() function to SQLite engine\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pythagore(lat_1, long_1, lat_2, long_2) -> float:\n",
    "    \"\"\"\n",
    "    Returns square distance between two location, given their latitude and longitude values,\n",
    "    using the Pythagore formula.\n",
    "    \n",
    "    Note: The distance is returned as the power of 2 of it, to reduce number of operations\n",
    "    The goal here is to find the nearest, not an absolute distance.\n",
    "    \n",
    "    I've decided to implement the pythagore() function in the SQLite connection object with\n",
    "    the sqltite3.Connection.create_function. More details on the SQLite3 documentation:\n",
    "    https://docs.python.org/2/library/sqlite3.html#sqlite3.Connection.create_function\n",
    "    \n",
    "    This pythagore function() will be used in different queries, that why I made it available\n",
    "    on the global connection object\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "    \n",
    "    \"\"\"\n",
    "    return (lat_1 - lat_2)**2 + (long_1 - long_2)**2\n",
    "\n",
    "\n",
    "# Add new SQL function to connection\n",
    "print(\"Add pythagore() function to SQLite engine\")\n",
    "con=sql_database_connection\n",
    "con.create_function('pythagore', 4, pythagore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table(tablename, sql_connection=None) -> bool:\n",
    "    \"\"\"\n",
    "    Drop a table registered in the SQL database.    \n",
    "    Returns True if DROP operation is succesfful, False otherwise\n",
    "    \n",
    "    The sql_connection parameter can be used to pass a connection object to a database where the saving\n",
    "    process should be done. If set to None (default value), the connection used is the one returned\n",
    "    by the get_sql_connection() function.\n",
    "    \n",
    "    \n",
    "    Code copied from: https://www.tutorialspoint.com/python_data_access/python_sqlite_drop_table.htm\n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get SQLite connection if not provided\n",
    "    if sql_connection==None:\n",
    "        sql_connection = get_sql_connection()\n",
    "               \n",
    "    #Creating a cursor object using the cursor() method\n",
    "    cursor = sql_connection.cursor()\n",
    "\n",
    "    try:\n",
    "        #Doping tablename table if already exists\n",
    "        cursor.execute(f'DROP TABLE IF EXISTS {tablename}')\n",
    "        print(f\"'{tablename}' table dropped from SQL database\")\n",
    "\n",
    "        #Commit your changes in the database\n",
    "        sql_connection.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error while dropping table: {tablename}\")\n",
    "        print(e)\n",
    "        return False\n",
    "               \n",
    "    # Everything is OK,\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sql(dataset, tablename = 'travel', sql_connection=None, if_exists = 'replace') -> bool:\n",
    "    \"\"\"\n",
    "    Dump dataset passed as parameter in an SQL table named 'tablename' of the SQLITE_DATAFILE.\n",
    "    \n",
    "    The 'if_exists' parameter is the same usage as in pandas.to_sql() method, it value is passed as is to the\n",
    "    pandas.to_sql() call.\n",
    "    \n",
    "    It will return a boolean value set to True if saving process was done completly.\n",
    "    \n",
    "    The sql_connection parameter can be used to pass a connection object to a database where the saving\n",
    "    process should be done. If set to None (default value), the connection used is the one returned\n",
    "    by the get_sql_connection() function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get SQLite connection if not provided\n",
    "    if sql_connection==None:\n",
    "        sql_connection = get_sql_connection()\n",
    "\n",
    "    # Write dataset content to SQL database\n",
    "    try:\n",
    "        dataset.to_sql(f'{tablename}', sql_connection, if_exists=if_exists, index=False) # index=False\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] while writting data to SQL datafile\")\n",
    "        print(e)\n",
    "        return False\n",
    "    finally:\n",
    "        print(\"Saving OK\")\n",
    "        return True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sql(tablename='travel', query=None, limit=None, offset=0, verbose=True, sql_connection=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \n",
    "    Load data from the SQL database and returns result as a dataframe.\n",
    "    \n",
    "    This function has two different behaviour, depending on the 'query' parameter.\n",
    "    \n",
    "    - query=None: In that situation, the whole content of the 'tablename' is loaded into a pd.Dataframe using\n",
    "      the following query: 'SELECT * FROM {tablename}'\n",
    "      \n",
    "    - query=A valid SQL query: In that case, the dataframe returned by the function is filled in with the data\n",
    "      rtrived using the 'query' SQL command. THe 'tablename' parameter is ignored.\n",
    "      \n",
    "    It accepts two optional int parameter, limit and offset, which if set, append 'LIMIT limit' and 'OFFSET offest' to the query.\n",
    "    This is mostly used to retrieve a small subset of the table or query.\n",
    "    Of course, the 'OFFSET' directive is only set if 'LIMIT' is set.\n",
    "    \n",
    "    Verbose parameter, is set to True, will display the query that will be used. Default value is True\n",
    "    \n",
    "    The sql_connection parameter can be used to pass a connection object to a database where the saving\n",
    "    process should be done. If set to None (default value), the connection used is the one returned\n",
    "    by the get_sql_connection() function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Get SQLite connection if not provided\n",
    "    if sql_connection==None:\n",
    "        sql_connection = get_sql_connection()\n",
    "    \n",
    "    # Initialize empty variable to store resulting dataframe\n",
    "    df=None\n",
    "    \n",
    "    # Check 'query' parameter\n",
    "    if query==None:\n",
    "        query=f'SELECT * FROM {tablename}'\n",
    "        \n",
    "    # Check 'limit' parameter\n",
    "    if limit!=None:\n",
    "        query+=' LIMIT {} OFFSET {}'.format(limit, offset)\n",
    "    \n",
    "    # Print query if verbose=True\n",
    "    if verbose==True:\n",
    "        print(\"Query: {}\".format(query))\n",
    "    # Run SQL 'query'\n",
    "    try:\n",
    "        df=pd.read_sql_query(query, sql_connection)\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] while running SQL query:\", query)\n",
    "        print(e)\n",
    "        return None\n",
    "    finally:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice_list(dataset, slice_interval=100000) -> list :\n",
    "    \"\"\"\n",
    "    Returns a list of tuples representing slices of lines of the dataset\n",
    "    passed as first parameter.\n",
    "    \n",
    "    The size of the slices is set by the 'slice_interval' parameter (default is 100'000)\n",
    "    \n",
    "    The dataset passed as parameter is used to determine the number of lines.\n",
    "    \n",
    "    The list of tuples returned looks like:\n",
    "      [(0,99999), (100000,199999), (200000, 299999), ..., (1400000, <number of lines>)]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # slices list that the function will return\n",
    "    intervals=[]\n",
    "\n",
    "    # Get number of lines\n",
    "    nb_lines=len(dataset.index)\n",
    "    \n",
    "    # set internal counter to 0\n",
    "    i=0\n",
    "    \n",
    "    # Loop while inernal counter is lower than number of lines\n",
    "    while i < nb_lines:\n",
    "        if i+slice_interval < nb_lines:\n",
    "            intervals.append((i, i+slice_interval-1))\n",
    "        else:\n",
    "            intervals.append((i,nb_lines))\n",
    "        i+=slice_interval\n",
    "\n",
    "    return(intervals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_npz(dataset, y_column='km_per_hour', y_dtype='float', npz_filename=NPZ_NORMALIZED_DATAFILE) -> None:\n",
    "    \"\"\"\n",
    "    Function used to save the dataset passed as parameter in an NPZ file, along with the datasets\n",
    "    created during the Data Preparation of this project.\n",
    "    \n",
    "    The parameter 'y_column' is used to define, in the dataset passed as parameter, wich feature is\n",
    "    the Y vector. Defaults to 'km_per_hour'. 'y_dtype' defines the type of the 'y_column'. Default to 'float'.\n",
    "    \n",
    "    The content of the NPZ file will be made of:\n",
    "    \n",
    "    - The dataset passed as parameter\n",
    "    - The categorical and numerical column names of this dataset, based on their dtype      \n",
    "    - The dependent variable name (as list with one element) and its dtype\n",
    "    - The *stations* dataset\n",
    "    - The *travel improved* dataset\n",
    "    - The *weather categorical improved* dataset\n",
    "    - The *weather numerical improved* dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get numerical feature names from dataset\n",
    "    numerical_features=dataset.head(1).drop(y_column, axis=1).select_dtypes('float').columns\n",
    "    print(\"Numerical features: \", ','.join(numerical_features))\n",
    "    \n",
    "    # Get categorical feature names from dataset\n",
    "    categorical_features=dataset.head(1).drop(y_column, axis=1).select_dtypes('int').columns\n",
    "    print(\"Categorical features: \", ','.join(categorical_features))\n",
    "    \n",
    "    # Build the dict that will be saved into NPZ file\n",
    "    # Add features\n",
    "    print(\"Build dict to pass to savez_compressed...\")\n",
    "    npz_dict={\n",
    "        'features_numerical'          : list(numerical_features),\n",
    "        'features_categorical'        : list(categorical_features),\n",
    "        'features_y'                  : [y_column],\n",
    "        'features_y_dtype'            : [y_dtype],\n",
    "\n",
    "        'dataset_full'                : dataset,\n",
    "        'dataset_full_columns'        : list(dataset.columns),\n",
    "\n",
    "        'dataset_stations'            : load_sql(STATION_TABLENAME),\n",
    "        'dataset_stations_columns'    : list(load_sql(STATION_TABLENAME, limit=1).columns),\n",
    "\n",
    "        'dataset_travel'              : load_sql(TRAVEL_TABLENAME),\n",
    "        'dataset_travel_columns'      : list(load_sql(TRAVEL_TABLENAME, limit=1).columns),\n",
    "\n",
    "        'dataset_weather_cat'         : load_sql(WEATHER_CAT_TABLENAME),\n",
    "        'dataset_weather_cat_columns' : list(load_sql(WEATHER_CAT_TABLENAME, limit=1).columns),\n",
    "\n",
    "        'dataset_weather_num'         : load_sql(WEATHER_NUM_TABLENAME),\n",
    "        'dataset_weather_num_columns' : list(load_sql(WEATHER_NUM_TABLENAME, limit=1).columns),\n",
    "\n",
    "    }\n",
    "\n",
    "    # Save npz_dict to an NPZ file\n",
    "    print(\"Save dict to NPZ file\", npz_filename)\n",
    "    np.savez_compressed(npz_filename, **npz_dict)\n",
    "\n",
    "    print(\"Process terminated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_as_dict(dataset='full', frac=1, random_state=RANDOM_STATE, verbose=True, y_dtype='float', npz_filename=NPZ_NORMALIZED_DATAFILE) -> dict:\n",
    "    \"\"\"\n",
    "    This function returns one of the dataset stored in the NPZ file passed as parameter,\n",
    "    and if the dataset claimed is the full one, then its feature names are added to the\n",
    "    dict returned by the function.\n",
    "    \n",
    "    The dict structure returned looks like this:\n",
    "        - features: (if requested dataset is the full one)\n",
    "            - numerical\n",
    "            - categorical\n",
    "            - all\n",
    "            - y\n",
    "        - dataset\n",
    "        - frac  \n",
    "    \n",
    "    The NPZ file passed should contain a Python dict built in Notebook No 17\n",
    "    \n",
    "    The dataset parameter is used to determine which dataset the function should return.\n",
    "    Dafault value is 'full'\n",
    "    \n",
    "    The frac parameter, if < 1, will be used as parameter to the sample() method to keep only\n",
    "    a fraction of the dataset. frac=0.1 means 10% of the lines.\n",
    "    \n",
    "    When frac is used, the random_state parameter could be used to change the random seed\n",
    "    of the sample() method. Default is 5.\n",
    "        \n",
    "    Note: This function takes care of reapplying dtypes to numerical and categorical columns\n",
    "    before returning dataframe (this is lost in save/load NPZ process)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize returned variable\n",
    "    ret_dict=dict()\n",
    "    \n",
    "    # Load the NPZ file\n",
    "    if verbose:\n",
    "        print(f\"Loading dataset '{dataset}' from NPZ file\", npz_filename)\n",
    "              \n",
    "    with np.load(npz_filename) as npz_file:\n",
    "        \n",
    "        # initialize retruned dict\n",
    "        ret_dict={\n",
    "            'features': dict(),\n",
    "            'dataset': None\n",
    "        }\n",
    "        \n",
    "        if dataset=='full':\n",
    "            # Load feature names\n",
    "            for feature in ['numerical', 'categorical', 'y']:\n",
    "                # Load features\n",
    "                ret_dict['features'][feature]=npz_file[f'features_{feature}']\n",
    "                \n",
    "                # Remap types. If len of features == 1, convert to str, list otherwise.\n",
    "                if(len(ret_dict['features'][feature]) == 1):\n",
    "                    ret_dict['features'][feature]=''.join(ret_dict['features'][feature])\n",
    "                else:\n",
    "                    ret_dict['features'][feature]=list(ret_dict['features'][feature])\n",
    "            # Load y_dtype\n",
    "            ret_dict['features']['y_dtype']=''.join(npz_file[f'features_y_dtype'])\n",
    "\n",
    "        ret_dict['features']['all']=list(npz_file[f'dataset_{dataset}_columns'])\n",
    "        ret_dict['dataset']=pd.DataFrame(npz_file[f'dataset_{dataset}'], columns=ret_dict['features']['all'])\n",
    "        \n",
    "        # Apply dtype to full dataset\n",
    "        if dataset=='full':\n",
    "            # Build dtype column dictionnary\n",
    "            if verbose:\n",
    "                print(\"Apply correct dtype to dataset column\")\n",
    "            column_dtype={ret_dict['features']['y']: ret_dict['features']['y_dtype']}\n",
    "            for col in ret_dict['features']['numerical']:\n",
    "                column_dtype[col] = 'float'\n",
    "            for col in ret_dict['features']['categorical']:\n",
    "                column_dtype[col] = 'int'\n",
    "            if verbose:\n",
    "                print(column_dtype)\n",
    "            \n",
    "            # Apply dtype to dataset columns\n",
    "            ret_dict['dataset']=ret_dict['dataset'].astype(column_dtype)         \n",
    "        \n",
    "        # sample dataframe if frac < 1\n",
    "        if verbose:\n",
    "            print(f\"Building sample from dataset (frac={frac})\")\n",
    "        if frac < 1:\n",
    "            ret_dict['dataset']=ret_dict['dataset'].sample(frac=frac, random_state=random_state)\n",
    "            \n",
    "        if verbose:\n",
    "            print(' Dataset shape: {}'.format(ret_dict['dataset'].shape))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Dataset loaded, returning dict\")\n",
    "    # Return the structure prepared by this function\n",
    "    return ret_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(frac=1, random_state=RANDOM_STATE, verbose=True, y_dtype='float', npz_filename=NPZ_NORMALIZED_DATAFILE) -> tuple:\n",
    "    \"\"\"\n",
    "    Convenient wrapper around the load_npz_as_dict() function that returns the full dataset, its feature and result vector name\n",
    "    as a tuple.\n",
    "    \n",
    "    This function exists to simplify the code when loading full dataset. For example, the follwing instruction\n",
    "    loads the full dataset store it in df variable, feature variable will contain a list of the rest of the tuple\n",
    "    \n",
    "        df,*features=load_dataset()\n",
    "        \n",
    "    Parameters are passed as is to the load_npz_as_dict() function.\n",
    "    \n",
    "    Returned tuple is:\n",
    "        - dataset\n",
    "        - all features names (without y)\n",
    "        - y result vector name\n",
    "        - numerical feature names\n",
    "        - categorical feature names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    npz_dict=load_npz_as_dict(frac=frac, random_state=random_state, verbose=verbose, y_dtype=y_dtype, npz_filename=npz_filename)\n",
    "    \n",
    "    npz_dict['features']['all'].remove(npz_dict['features']['y'])\n",
    "\n",
    "    return npz_dict['dataset'], npz_dict['features']['all'], npz_dict['features']['y'], npz_dict['features']['numerical'], npz_dict['features']['categorical']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy_as_dict(train_size=TRAIN_SIZE_DEFAULT, frac=1, random_state=RANDOM_STATE, verbose=True, npz_filename=NPZ_NORMALIZED_DATAFILE) -> dict:\n",
    "    \"\"\"\n",
    "    Used to get features and vector result of the 'full' dataset as X and y np.array, splitted into two daatset: A train\n",
    "    and valid one.\n",
    "    \n",
    "    The 'train_size' parameter may be used to fix the train size (defaults 0.8). This parameter is passed as is to the\n",
    "    'sklearn.model_selection.train_test_split()' method.\n",
    "    \n",
    "    The value returned is a dict object:\n",
    "    \n",
    "        - train:\n",
    "            - X:    Train set of X features\n",
    "            - y:    Train set of y vector result\n",
    "            \n",
    "        - valid:\n",
    "            - X:    Validation set of X features\n",
    "            - y:    Validation set of y vector result\n",
    "            \n",
    "        - all:\n",
    "            - X:    Complete set of X features\n",
    "            - y:    Complete set of y vector result\n",
    "            \n",
    "        - features: List of feature names\n",
    "        - result:   Name of the y result vector\n",
    "    \n",
    "    The 'full' dataset is retrived using the 'load_dataset()' function.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load 'full' dataset\n",
    "    if verbose:\n",
    "        print(\"Loading dataset...\")\n",
    "    df, x_col, y_col, *_=load_dataset(frac=frac, random_state=random_state, verbose=False, y_dtype='float', npz_filename=npz_filename)\n",
    "\n",
    "    # Split data in two dataframe    \n",
    "    if verbose:\n",
    "        print(\"Splitting dataset...\")    \n",
    "    tr_df, va_df = train_test_split(df, test_size = 1-train_size, train_size=train_size, random_state=random_state)\n",
    "\n",
    "    # initialize returned value\n",
    "    ret_dict={\n",
    "        'train'   : {\n",
    "            'X': tr_df.drop([y_col], axis=1).values,\n",
    "            'y': tr_df[y_col].values\n",
    "        },\n",
    "        'valid'   : {\n",
    "            'X': va_df.drop([y_col], axis=1).values,\n",
    "            'y': va_df[y_col].values\n",
    "        },\n",
    "        'all'     : {\n",
    "            'X': df.drop([y_col], axis=1).values,\n",
    "            'y': df[y_col].values\n",
    "        },\n",
    "        'features': x_col,\n",
    "        'result'  : y_col\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Load and split process terminated\")    \n",
    "        \n",
    "        print(\"  Shape of X train variable:\", ret_dict['train']['X'].shape)\n",
    "        print(\"  Shape of y train variable:\", ret_dict['train']['y'].shape)\n",
    "\n",
    "        print(\"  Shape of X valid variable:\", ret_dict['valid']['X'].shape)\n",
    "        print(\"  Shape of y valid variable:\", ret_dict['valid']['y'].shape)\n",
    "\n",
    "        print(\"  Shape of X variable   :\", ret_dict['all']['X'].shape)\n",
    "        print(\"  Shape of y variable   :\", ret_dict['all']['y'].shape)\n",
    "        \n",
    "    return ret_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy(train_size=TRAIN_SIZE_DEFAULT, frac=1, random_state=RANDOM_STATE, verbose=True, npz_filename=NPZ_NORMALIZED_DATAFILE) -> tuple:\n",
    "    \"\"\"\n",
    "    A wrapper function around load_Xy_as_dict() that returns X_tr, y_tr, X_va and y_va as a tuple.\n",
    "    \n",
    "    This function aims to simplify the code in Notebooks\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    (X_tr, y_tr, X_va, y_va)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data=load_Xy_as_dict(train_size=train_size, frac=frac, random_state=random_state, verbose=False, npz_filename=npz_filename)\n",
    "    \n",
    "    return data['train']['X'], data['train']['y'], data['valid']['X'], data['valid']['y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Station functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stations(station_list=[]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the 'weather' dataset and returns a dataframe containing the list of stations.\n",
    "    \n",
    "    'list' parameter can be used to get a subset of the stations dataset. This parameter is a list\n",
    "    and must contains the name of the STATION we'd like to retrieve from database.\n",
    "    \n",
    "    This list is taken from the SQL Database.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if(len(station_list) > 0):\n",
    "        query=\"select * from stations where STATION IN ('{}')\".format(\"','\".join(station_list))\n",
    "        return load_sql(query=query, verbose=False)\n",
    "    else:\n",
    "        return load_sql('stations', verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_station_from_location(latitude, longitude) -> str:\n",
    "    \"\"\"\n",
    "    Returns the nearest station id from the latitude and longitude position received as parameter\n",
    "    \n",
    "    This function assumes that the SQL Table stations exists.\n",
    "    !! Do not use this function before running the 83 Weather Stations notebook\n",
    "    \n",
    "    To get the nearest station, I will use Pythagore in an SQL query to get the distance between stations and\n",
    "    the designated location, sort by the distance in descending order and return the first station returned\n",
    "    by the query.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    query=\"SELECT station, latitude, longitude, pythagore(latitude, longitude, {}, {}) as distance FROM stations\".format(latitude, longitude)\n",
    "    query += \" ORDER BY distance ASC LIMIT 1\"\n",
    "\n",
    "    # Get connection to SQL database\n",
    "    con=get_sql_connection()\n",
    "    \n",
    "    # Return STATION cell of the first dataframe row\n",
    "    return load_sql(query=query, sql_connection=con, verbose=False).loc[0]['STATION']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rectangle(lower_left=[1,1], upper_right=[2,2], color='r', linestyle='-', linewidth=1 ):\n",
    "    \"\"\"\n",
    "    Draw a rectangle in a pyplot canvas, based on two points:\n",
    "    - downleft is the lower left corner of the rectangle defined as a [x,y]\n",
    "    - upperright is the upper right corner of the rectangle\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.plot([lower_left[0], lower_left[0]], [lower_left[1], upper_right[1]], color=color, linestyle=linestyle, linewidth=linewidth)\n",
    "    plt.plot([upper_right[0], upper_right[0]], [lower_left[1], upper_right[1]],color=color, linestyle=linestyle, linewidth=linewidth)\n",
    "    plt.plot([lower_left[0], upper_right[0]], [lower_left[1], lower_left[1]],color=color, linestyle=linestyle, linewidth=linewidth)\n",
    "    plt.plot([lower_left[0], upper_right[0]], [upper_right[1], upper_right[1]],color=color, linestyle=linestyle, linewidth=linewidth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nyc_map_geodataframe() -> geopandas.geodataframe.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Return the Geopandas Dataframe that will be used to draw the nyc map.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    geopandas.geodataframe.GeoDataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Return the GeoDataFrame of New-York\n",
    "    return geopandas.read_file('data/nyu_2451_34490')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_nyc_map(latitude=[], longitude=[]):\n",
    "    \"\"\"\n",
    "    Minimalist function that draws a map of New-York city\n",
    "    \n",
    "    The map definition is based on the shape provided by the University of Texas in Austin:\n",
    "    https://archive.nyu.edu/retrieve/74689/nyu_2451_34490.zip\n",
    "    \n",
    "    TODO: Add parameters to draw plots on the map.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Load shape of New-York\n",
    "    nyc_df = get_nyc_map_geodataframe()\n",
    "\n",
    "    # Convert position to the same metric system of our datasets\n",
    "    nyc_df=nyc_df.to_crs(epsg=4326)\n",
    "    \n",
    "    if len(latitude) == 0:\n",
    "        \n",
    "        nyc_df.plot(color='lightblue', edgecolor='black', figsize=(10,10), alpha=0.5)\n",
    "\n",
    "    else:\n",
    "       # Build dataframe with latitude and longitude values provided\n",
    "        points = pd.DataFrame(\n",
    "            {\n",
    "                'Latitude': latitude,\n",
    "                'Longitude': longitude\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Initialize data point to draw on the map\n",
    "        gdf = geopandas.GeoDataFrame(points, geometry=geopandas.points_from_xy(points.Longitude, points.Latitude)) \n",
    "\n",
    "        # Set drawing options\n",
    "        ax = nyc_df.plot(color='lightblue', edgecolor='black', figsize=(10,10), alpha=0.5)\n",
    "\n",
    "        # Draw the map with data points\n",
    "        gdf.plot(ax=ax, color='red')\n",
    "\n",
    "\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_correlation_matrix(dataset, title='Correlation Matrix', figsize=(10,10), fontsize=10,\n",
    "                                sns_style='darkgrid', center=0.5) -> None:\n",
    "    \"\"\"\n",
    "    Draw a correlation matrix using a Seaborn heatmap for better visual search of data correlation\n",
    "    \n",
    "    Expects a dataset as parameter, and optional features like title, figsize, fontsize, and sns style.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Set figure figsize parameter\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Draw heatmap\n",
    "    sns.heatmap(dataset.corr(), annot=True, center=center, cmap='Blues')\n",
    "    \n",
    "    # Set title\n",
    "    plt.title(title, weight=\"bold\", fontsize=2*fontsize, pad=30)\n",
    "    \n",
    "    # Set X and Y labels font size\n",
    "    plt.xticks(weight=\"bold\", fontsize=fontsize)\n",
    "    plt.yticks(weight=\"bold\", fontsize=fontsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter_plot(dataset, x_columns, y_column, z_factor=2, graph_per_line=4, figsize=(25,15), ylim=[0,2], polyfit_deg=0) -> None:\n",
    "    '''\n",
    "    Function that draws scatter plots from the dataframe passed as first parameter (df), using the column name passed parameter\n",
    "    x_column as X-axis, and column name passed as second y_column parameter as Y-axis. Number of Scatter plots equals the\n",
    "    lenght of the x_column parameter, the 'graph_per_line' parameter is used to fix how many plots are displayed per line.\n",
    "    The number of lines of scatter plots is equal to \"( (len(x_column)-1) / graph_per_line ) + 1\". Trust me ;-)\n",
    "    \n",
    "    Before plotting the result, this function will remove lines where x values are == 0, and remove outliers on both columns\n",
    "    using a 'zscore' approach. The z_factor can be set using the 'z_factor' parameter.\n",
    "    \n",
    "    Figure size and Y-axis limit can be set using 'figsize' and 'y-limit' parameter.\n",
    "    \n",
    "    The last parameter, 'polyfit_deg', if > 0 determine the degrees of polyfit to be calculated and drawn along with the scatter plots.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \n",
    "    '''\n",
    "\n",
    "    nb_columns=len(x_columns)\n",
    "    nb_line=int(((nb_columns-1)/graph_per_line)+1)\n",
    "    \n",
    "    print(\"Drawing graphs {} x {} (number of x_columns = {})\".format(graph_per_line, nb_line, nb_columns))\n",
    "    \n",
    "    graph_col=0\n",
    "    graph_line=0\n",
    "\n",
    "    \n",
    "    # Create Figure and Axes instances\n",
    "    if(nb_line > 1):\n",
    "        fig, axs = plt.subplots(nb_line, graph_per_line, sharey=True, figsize=figsize)\n",
    "    else:\n",
    "        fig, axs = plt.subplots(1, graph_per_line, figsize=figsize)\n",
    "        \n",
    "\n",
    "    for c in x_columns:\n",
    "\n",
    "        # Log information\n",
    "        print(f'Plotting feature: {c}')\n",
    "        \n",
    "        # Remove 0 values\n",
    "        temp_df = dataset[dataset[c]!=0][[c, y_column]]\n",
    "    \n",
    "        # Calculate zscore of each cells of the columns passed as parameter\n",
    "        zscore_values = zscore(temp_df[[c,y_column]])\n",
    "    \n",
    "        # Build an arraqy of boolen where True = np.abs(zscore) is lower than factor\n",
    "        zscore_bool = np.abs(zscore_values) < z_factor\n",
    "    \n",
    "        # Use the all(axis=1) function on zscore_bool array to get a 1-row-array containing boolean values\n",
    "        # where True means that at least one cell of the zscore_bool corresponding line is True\n",
    "        # This 1-row-array will be used to filter the temp dataframe\n",
    "        temp_df = temp_df[zscore_bool.all(axis=1)]\n",
    "    \n",
    "        # Make plot, set axes labels\n",
    "        if(nb_line>1):\n",
    "            ax=axs[graph_line, graph_col]\n",
    "        else:\n",
    "            ax=axs[graph_col]\n",
    "\n",
    "        x=temp_df[c]\n",
    "        y=temp_df[y_column]\n",
    "        \n",
    "        ax.scatter(x, y, s=10, c=y, cmap='viridis')\n",
    "        ax.set_ylabel(y_column)\n",
    "        ax.set_xlabel(\"{} - ({} lines)\".format(c, x.shape[0]))\n",
    "        ax.set_title(\"({}) / ({})\".format(c,y_column))\n",
    "        ax.set_ylim(ylim)\n",
    "        \n",
    "        \n",
    "        # Draw polyfit from deg=1 to deg=polyfit_deg\n",
    "        # Do nothing if polyfit_deg == 0 or > 3\n",
    "        if polyfit_deg > 0 and polyfit_deg < 4:\n",
    "            colors=['r','b','g']\n",
    "        \n",
    "            \n",
    "            for i,color in enumerate(colors[:polyfit_deg]):\n",
    "                coef=np.polyfit(x,y,deg=i+1)\n",
    "                x_values=np.linspace(x.min(), x.max())\n",
    "                y_values=np.polyval(coef, x_values)\n",
    "                ax.plot(x_values, y_values, label='poly(deg={})'.format(i+1), c=color)\n",
    "                ax.legend()\n",
    "        \n",
    "        # increment axes index\n",
    "        graph_col+=1\n",
    "        if(graph_col > graph_per_line-1):\n",
    "            graph_col=0\n",
    "            graph_line+=1\n",
    "\n",
    "    # set the spacing between subplots\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                        bottom=0.1, \n",
    "                        right=0.9, \n",
    "                        top=0.9, \n",
    "                        wspace=0.2, \n",
    "                        hspace=0.3)\n",
    "    \n",
    "    print(\"Processing done, display result (may take some time)\")\n",
    "    \n",
    "    # display graph\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime & Distance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(start='2016-01-01', end='2016-06-30') -> list:\n",
    "    \"\"\"\n",
    "    Functions used to get a list of days, encoded as '%Y-%m-%d', between a start day and an end day.\n",
    "    \n",
    "    Start and end day are passed as parameters to the function. Default values are:    \n",
    "    - start : '2016-01-01'\n",
    "    - end   : '2016-06-30'\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list    \n",
    "    \n",
    "    \"\"\"\n",
    "    day_list=[]\n",
    "    start_date=datetime.strptime(start, '%Y-%m-%d')\n",
    "    end_date=datetime.strptime(end, '%Y-%m-%d')\n",
    "        \n",
    "    while (start_date <= end_date):\n",
    "        day_list.append(start_date.strftime('%Y-%m-%d'))\n",
    "        start_date=start_date+timedelta(days=1)\n",
    "\n",
    "    return day_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(value) -> int:\n",
    "    \"\"\"\n",
    "    Returns 1 if the day of week of the datetijme64 value received as parameter is\n",
    "    on a week-end (Saturday or Sunday)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get day of week\n",
    "    dayofweek=value.dayofweek\n",
    "    \n",
    "    # Check day of week value and return 1 or 0\n",
    "    if dayofweek < 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_category(value):\n",
    "    \"\"\"\n",
    "    Returns the category of the time value received as parameter.\n",
    "    Note that the parameter format is pandas.dattime[64]\n",
    "    \n",
    "    Categories are morning, afternoon, evening, night\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve hour from datetime parameter\n",
    "    hour=int(value.strftime('%H'))\n",
    "    \n",
    "    if hour < 6:\n",
    "        return 'night'\n",
    "    elif hour <12:\n",
    "        return 'morning'\n",
    "    elif hour < 18:\n",
    "        return 'afternoon'\n",
    "    elif hour < 22:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_in_km(lat1, lon1, lat2, lon2) -> pd.core.series.Series:\n",
    "        \"\"\"\n",
    "        Python 3 program to calculate Distance Between Two Points on Earth, based\n",
    "        on latitude and longitude in degrees.\n",
    "        \n",
    "        It expects latitude and longitude pandas.Series of the two datapoints in degrees, and returns\n",
    "        the distance between them in kilometers as a pandas.Series\n",
    "        \n",
    "        The code of this function has been copied from https://www.geeksforgeeks.org/program-distance-two-points-earth/\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.core.series.Series\n",
    "        \n",
    "        \"\"\"\n",
    "        # The math module contains a function named\n",
    "        # radians which converts from degrees to radians.\n",
    "        lon1 = np.radians(lon1)\n",
    "        lat1 = np.radians(lat1)\n",
    "\n",
    "        lon2 = np.radians(lon2)\n",
    "        lat2 = np.radians(lat2)\n",
    "\n",
    "        # Haversine formula\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "        # Radius of earth in kilometers. Use 3956 for miles\n",
    "        r = 6371\n",
    "\n",
    "        # calculate the result\n",
    "        return(c * r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of the dataset used to train models: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# Display fraction of the datasets used to train models\n",
    "print(\"Fraction of the dataset used to train models: {:.2f}%\".format(FRAC_VALUE_FOR_ML*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_filename(model_name) -> str:\n",
    "    \"\"\"\n",
    "    Basic function that will return the filename used to store on disk\n",
    "    the model passed as parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "    \n",
    "    \"\"\"\n",
    "    return os.path.join(DATA_PATH, f'model-{model_name}.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def save_model(model, name) -> None:\n",
    "    \"\"\"\n",
    "    Function that saves on disk the fitted model passed as first\n",
    "    parameter using pickle or keras library, depending on model type\n",
    "    It uses the function getModelFilename() with the 'name'\n",
    "    parameter to get the filename where to save the model.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    filename=get_model_filename(name)\n",
    "    # Save model to disk\n",
    "    if isinstance(model, keras.Sequential):\n",
    "        print(\"Saving model {} to {} using 'keras.models.save_model' library\".format(name, filename))\n",
    "        keras.models.save_model(model, filename, overwrite=True)\n",
    "    else:\n",
    "        print(\"Saving model {} to {} using 'pickle' library\".format(name, filename))\n",
    "        pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    \"\"\"\n",
    "    Function that loads from disk the model of which name is passed\n",
    "    as first parameter. It uses the function getModelFilename() with\n",
    "    the 'name' parameter to get the filename from where to load the model.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Fitted model\n",
    "    \n",
    "    \"\"\"\n",
    "    filename=get_model_filename(name)\n",
    "    # load the model from disk\n",
    "    print(\"Loading model from \", filename)\n",
    "    model=None\n",
    "    try:\n",
    "        model=pickle.load(open(filename, 'rb'))\n",
    "        print(\"Model loaded using pickle()\")\n",
    "    except:\n",
    "        model=keras.models.load_model(filename)\n",
    "        print(\"Model loaded using keras.models.load_model()\")\n",
    "    finally:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_pred, y) -> np.array:\n",
    "    \"\"\"\n",
    "    Returns 10^mean_absolute_error() between the two result vector passed as parameter.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    10^(mean absolute error)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return 10**mean_absolute_error(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_pred, y) -> float:\n",
    "    \"\"\"\n",
    "    Define a performance metric in percentage\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mean absolute percentage score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Return percentage value\n",
    "    return 100 - np.mean(100 * (mean_absolute_error(y_pred, y) / y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom scoring function using mape()\n",
    "custom_scorer = make_scorer(mape, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search_results(results_df,\n",
    "                             x_param,\n",
    "                             y_param=['mean_test_score', 'mean_train_score'],\n",
    "                             semilogx=True,\n",
    "                             xlabel='',\n",
    "                             ylabel='Score (%)',\n",
    "                             title='GridSearch results',\n",
    "                             figsize=(15,10),\n",
    "                             std_params={'mean_test_score': 'std_test_score'},\n",
    "                             std_factor=1,\n",
    "                             show_best_result=['mean_test_score'],\n",
    "                             greater_is_best=False\n",
    "                            ) -> None:\n",
    "    \"\"\"\n",
    "    Function to graph data points from GridSearchCV results., used to graph\n",
    "    the mean test score of a GridSearchCV fitted object.\n",
    "    \n",
    "    Mandatory parameters are:\n",
    "        results_df: A dataframe built from GridSearchCV.cv_results_ property\n",
    "        x_param: The column name of the results_df dataframe to be used as X axis\n",
    "        y_param: An array of column to be plotted on the Y axis.\n",
    "        \n",
    "    Optionnal parameters:\n",
    "        semilogx: If True, the X data points are plotted using a log10 scale\n",
    "        xlabel: Label of the X axis\n",
    "        ylabel: Label of the Y axis\n",
    "        title: Title of the graph\n",
    "        figsize: Size ot the graph\n",
    "        std_param: A dict with key=y_param element and value the corresponding\n",
    "                   standard deviation column name.\n",
    "                   This parameters is used to draw the std deviation of the\n",
    "                   y_params as a filled area around the data plot\n",
    "        std_factor: This parameter is used to amplify the standard deviation\n",
    "                    when building the std dev filled area. Default value is 1 and\n",
    "                    changing increasing it allows displaying standard 'small'\n",
    "                    deviation behaviours.\n",
    "                    Be warn that when changing this parameter to a value other\n",
    "                    that 1, the filled area does not represent absolute values\n",
    "                    but a trend of it.\n",
    "            \n",
    "    The function will also determine, for each of the y_param to be plotted,\n",
    "    which is the plot with the highest y_param value, and use the coordinates\n",
    "    to draw a red cross on the plotted line, along with horizontal and vertical\n",
    "    lines to the X and Y axis.\n",
    "    \n",
    "    For that purpose, the function first sort the results_df dataframe using\n",
    "    the x_param column in ascending order.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    # Order dataframe by xparam value\n",
    "    temp_df=results_df.sort_values(x_param)\n",
    "\n",
    "    # Define figsize\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Store x_min, x_max, y_min and y_max values to set xlimit and ylimit of the graph\n",
    "    x_min=0\n",
    "    y_min=100\n",
    "    x_max=0\n",
    "    y_max=0\n",
    "    \n",
    "    # Loop for each yparam plot\n",
    "    for i in y_param:\n",
    "        # Find indices of the best y value\n",
    "        if greater_is_best==True:\n",
    "            best_idx=temp_df[i].idxmin()\n",
    "        else:\n",
    "            best_idx=temp_df[i].idxmax()\n",
    "\n",
    "        # Get best x information\n",
    "        best_x = temp_df[x_param][best_idx]\n",
    "        # Get x plots\n",
    "        x_values=temp_df[x_param].astype('float64')\n",
    "        # Store x_min and x_max if needed\n",
    "        if x_min>np.min(x_values):\n",
    "            x_min=np.min(x_values)\n",
    "        if x_max<np.max(x_values):\n",
    "            x_max=np.max(x_values)\n",
    "\n",
    "\n",
    "\n",
    "        # get best y information\n",
    "        best_y=temp_df[i][best_idx] # Multiply by 100 to get %\n",
    "        # Get y plots\n",
    "        y_values=temp_df[i]\n",
    "        # Store y_min and y_max if needed\n",
    "        if y_min>np.min(y_values):\n",
    "            y_min=np.min(y_values)-y_values.std()\n",
    "        if y_max<abs(np.max(y_values)):\n",
    "            y_max=np.max(y_values)+y_values.std()\n",
    "\n",
    "        if semilogx:\n",
    "            plt.semilogx(x_values, y_values, label=i)\n",
    "        else:\n",
    "            plt.plot(x_values, y_values, label=i)\n",
    "\n",
    "        # Draw a cross on the best_x/best_accuracy point\n",
    "        if i in show_best_result:\n",
    "            # Write near of the cross the best_y/best_y value\n",
    "            plt.scatter(best_x, best_y, marker='x', c='red', zorder=10, label=f'{i} best result')\n",
    "            plt.text(best_x, best_y, 'x:{:.3f} y:{:.3f}'.format(best_x, best_y), color='red')\n",
    "\n",
    "            # Draw vertical/horizontal line to help read best score\n",
    "            plt.axvline(best_x, color='grey', linestyle='--')\n",
    "            plt.axhline(best_y, color='grey', linestyle='--')\n",
    "\n",
    "        #plt.plot([np.min(x_values), best_x], [best_y, best_y], c='red', alpha=0.5, linestyle='--')\n",
    "        \n",
    "        # Do we have any std_param set for this loop ?\n",
    "        for key in std_params:\n",
    "            if key==i:\n",
    "                # Get the std deviation values from std_param column name\n",
    "                std_values=temp_df[std_params[key]]\n",
    "                # plot a filled area to represent the standard deviation\n",
    "                if std_factor==1:\n",
    "                    label=std_params[key]\n",
    "                else:\n",
    "                    label='{} x {}'.format(std_params[key], std_factor)\n",
    "                plt.fill_between(x_values, y_values+std_factor*std_values, y_values-std_factor*std_values, alpha=0.4, label=label)\n",
    "                       \n",
    "    plt.title(title)\n",
    "    if semilogx:\n",
    "        xlabel=f'log({xlabel})'\n",
    "    plt.xlabel(xlabel)        \n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(bottom=y_min, top=y_max)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_mape_per_speed_interval(dataset, columns, y='y', start=5, stop=50, step=5, title=None, figsize=(15,10), barwidth=1.5, draw_limit=True) -> None:\n",
    "    \"\"\" \n",
    "    Draw a graph of the MAPE values per speed interval, using columns and\n",
    "    y features from dataset passed as first paramters.\n",
    "    Speed interval start, stop and step are optional parameters using\n",
    "    default values.\n",
    "    \n",
    "    title, an optionnal parameter, may be used to set the graph title.\n",
    "    If set to None, a default title is set.\n",
    "    \n",
    "    figsize is used to define the size of the graph. Default is (10,6)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Plot index and result to get MAPE per km/h interval\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Offset to center bar on xticks\n",
    "    offset=barwidth/2*(1-len(columns))\n",
    "    \n",
    "    # Initialize best_y to 0 and poor_y to 100\n",
    "    best_y=0\n",
    "    best_model=''\n",
    "    poor_y=100\n",
    "    poor_model=''\n",
    "    \n",
    "    for k in columns:\n",
    "        \n",
    "        # Build dataframe with y_pred and y_va\n",
    "        df=pd.DataFrame({'y_pred': dataset[k], 'y_va': dataset[y]})\n",
    "\n",
    "        # result variables\n",
    "        index=[]\n",
    "        result=[]\n",
    "        label=[]\n",
    "\n",
    "        # For loop in interval from start to stop, using step increment\n",
    "        for i in range(start, stop, step):\n",
    "\n",
    "            # filter df to get data from interval\n",
    "            filter=np.logical_and(\n",
    "                df['y_va'] >= np.log10(i),\n",
    "                df['y_va'] <  np.log10(i+5)\n",
    "            )\n",
    "\n",
    "            # Append interval to index\n",
    "            index.append(i)\n",
    "\n",
    "            # Build index label\n",
    "            label.append(f'{i} to {i+5} km/h')\n",
    "\n",
    "            # Append MAPE to result, set to None if no data from filter\n",
    "            if(len(df[filter]['y_va'])>0):\n",
    "                result.append(mape(df[filter]['y_pred'], df[filter]['y_va']))\n",
    "            else:\n",
    "                result.append(None)\n",
    "\n",
    "        # Build dataframe with index and result\n",
    "        result_df=pd.DataFrame({'interval': index, 'mape': result})\n",
    "\n",
    "        # plt.plot(result_df['interval'], result_df['mape'])\n",
    "        plt.bar(result_df['interval'] + offset, result_df['mape'], width=barwidth, label=k)\n",
    "\n",
    "        # Increase offset\n",
    "        offset+=barwidth\n",
    "        \n",
    "        # Update best results\n",
    "        best=result_df.sort_values('mape', ascending=False).values[0][1]\n",
    "        if best > best_y:\n",
    "            best_y=best\n",
    "            best_model=k\n",
    "        \n",
    "        # Update poor results \n",
    "        poor=result_df.sort_values('mape', ascending=True).values[0][1]\n",
    "        if poor < poor_y:\n",
    "            poor_y=poor\n",
    "            poor_model=k\n",
    "            \n",
    "    # Draw horizontal line at best score\n",
    "    plt.axhline(y=best_y, color='green', linestyle='--', label='Best: {:.1f} % ({})'.format(best_y, best_model))\n",
    "    #plt.text(11, best_y-5, '{:.2f}%'.format(best_y), color='green')\n",
    "\n",
    "    # Draw horintal line at the lowest score\n",
    "    plt.axhline(y=poor_y, color='red', linestyle='--', label='Worst: {:.1f} % ({})'.format(poor_y, poor_model))\n",
    "    #plt.text(42, poor_y-5, '{:.2f}%'.format(poor_y), color='r')\n",
    "\n",
    "\n",
    "    # Set X and Y axis ticks and labels\n",
    "    plt.xticks(ticks=index, labels=label)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    else:\n",
    "        plt.title('Mean Absolute Percent Error (%) per speed interval')\n",
    "    plt.xlabel('Speed interval in km/h')\n",
    "    plt.ylabel('Mean Average Percent Error')\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # Show graph\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_utils library loaded :-)\n"
     ]
    }
   ],
   "source": [
    "print(\"my_utils library loaded :-)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
